{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f578d95",
   "metadata": {},
   "source": [
    "\n",
    "# Streaming Multimodal Outputs with Hugging Face Transformers\n",
    "**Based on:** *How to Stream Multimodal Outputs with Hugging Face Transformers (Bonus: Gradio Integration)* — Youssef Ghaoui (Medium)\n",
    "\n",
    "\n",
    "This notebook reproduces the core examples from the article: a blocking (traditional) generation example, a streaming generation example using `TextIteratorStreamer`, and a Gradio demo integration. Replace `HF_TOKEN` and model/device settings as needed for your environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ea6e22",
   "metadata": {},
   "source": [
    "\n",
    "## Environment / Install\n",
    "Run the following in a cell (or your terminal) to install required packages. Use the correct CUDA/torch combo for your machine.\n",
    "```bash\n",
    "# Example (adjust torch / cuda versions for your setup)\n",
    "%pip install transformers==4.50.1 accelerate==0.26.0 gradio==4.44.1 Pillow requests\n",
    "```\n",
    "**Note:** The notebook uses `google/gemma-3-4b-it` as an example; ensure you have access and sufficient GPU memory. Consider using a smaller model for testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19356dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imports and Hugging Face token setup\n",
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()  # load HF_TOKEN from .env if present\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\", None)\n",
    "if HF_TOKEN is None:\n",
    "    print(\"Warning: HF_TOKEN not found in environment. Set HF_TOKEN in your environment or .env to access private models if needed.\")\n",
    "\n",
    "# Common imports\n",
    "import torch\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cb835f",
   "metadata": {},
   "source": [
    "## 1) Traditional (blocking) generation — example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e5a645",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Traditional generation (blocks until finished)\n",
    "from transformers import Gemma3ForConditionalGeneration, AutoProcessor\n",
    "\n",
    "model_id = \"google/gemma-3-4b-it\"  # example\n",
    "# NOTE: for quick testing, replace with a small text-only model like 'gpt2' or similar\n",
    "try:\n",
    "    model = Gemma3ForConditionalGeneration.from_pretrained(model_id, device_map=\"auto\", token=HF_TOKEN, torch_dtype=torch.bfloat16).eval()\n",
    "    processor = AutoProcessor.from_pretrained(model_id)\n",
    "except Exception as e:\n",
    "    print(\"Model load error (expected in small/demo env):\", e)\n",
    "    model = None\n",
    "    processor = None\n",
    "\n",
    "# Prepare a sample multimodal message (image url + instruction)\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]},\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"},\n",
    "        {\"type\": \"text\", \"text\": \"Describe this image in detail.\"}\n",
    "    ]}\n",
    "]\n",
    "\n",
    "if model is not None and processor is not None:\n",
    "    inputs = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors='pt', use_fast=True, do_sample=False)\n",
    "    inputs = {k: (v.to(model.device, dtype=torch.bfloat16) if v.is_floating_point() else v.to(model.device)) for k, v in inputs.items()}\n",
    "    with torch.inference_mode():\n",
    "        generation = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n",
    "    decoded = processor.decode(generation[0][inputs['input_ids'].shape[-1]:], skip_special_tokens=True)\n",
    "    print('Decoded output:\\n', decoded)\n",
    "else:\n",
    "    print('Model not loaded. Replace model_id with a test-friendly model or run this on a GPU instance with enough memory.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b494b1de",
   "metadata": {},
   "source": [
    "## 2) Streaming generation with `TextIteratorStreamer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a100c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Streaming generation example using TextIteratorStreamer\n",
    "from transformers import TextIteratorStreamer, AutoTokenizer\n",
    "from threading import Thread\n",
    "\n",
    "# Ensure tokenizer is available for streaming decoding\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "except Exception as e:\n",
    "    tokenizer = None\n",
    "    print(\"Tokenizer load warning:\", e)\n",
    "\n",
    "# Load a real image into PIL\n",
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"\n",
    "image = None\n",
    "try:\n",
    "    image = Image.open(BytesIO(requests.get(url).content)).convert(\"RGB\")\n",
    "    print(\"Loaded image size:\", image.size)\n",
    "except Exception as e:\n",
    "    print(\"Could not load image from URL:\", e, \"— continuing with example code.\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]},\n",
    "    {\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": image}, {\"type\": \"text\", \"text\": \"Describe this image in detail.\"}]}\n",
    "]\n",
    "\n",
    "if model is not None and processor is not None and tokenizer is not None:\n",
    "    inputs = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors='pt', use_fast=True, do_sample=False)\n",
    "    # Move tensors to correct device/dtype\n",
    "    inputs = {k: (v.to(model.device, dtype=torch.bfloat16) if v.is_floating_point() else v.to(model.device)) for k, v in inputs.items()}\n",
    "\n",
    "    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True)\n",
    "    thread = Thread(target=model.generate, kwargs=dict(**inputs, streamer=streamer, max_new_tokens=256, temperature=0.7))\n",
    "    thread.start()\n",
    "\n",
    "    print('Streaming output:')\n",
    "    for token in streamer:\n",
    "        print(token, end='', flush=True)\n",
    "    print('\\n--- stream finished ---')\n",
    "else:\n",
    "    print('Streaming demo skipped because model/processor/tokenizer not loaded in this environment.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c22a90",
   "metadata": {},
   "source": [
    "## 3) Gradio integration (real-time UI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b435fb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Gradio streaming integration example (blocks when launched)\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "\n",
    "# IMPORTANT: Running this cell will launch a Gradio server. Stop it to continue using the notebook.\n",
    "# The code below follows the structure in the original article and uses streaming via TextIteratorStreamer.\n",
    "\n",
    "stop_flag = False\n",
    "max_size = 1024\n",
    "\n",
    "def process_image(image_input, image_url, stop=False):\n",
    "    global stop_flag\n",
    "    stop_flag = False\n",
    "\n",
    "    def stop_inference():\n",
    "        global stop_flag\n",
    "        stop_flag = True\n",
    "        print(\"Inference stopped. (stop_flag True)\")\n",
    "\n",
    "    if stop == 'Stop':\n",
    "        stop_inference()\n",
    "        return \"Inference stopped.\", None\n",
    "\n",
    "    # Load image from URL or the uploaded numpy image\n",
    "    if image_url:\n",
    "        try:\n",
    "            response = requests.get(image_url)\n",
    "            image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "        except Exception as e:\n",
    "            return f\"Error loading image from URL: {e}\", None\n",
    "    elif image_input is None:\n",
    "        return \"Cleared output\", None\n",
    "    else:\n",
    "        image = Image.fromarray(image_input.astype('uint8'))\n",
    "\n",
    "    # Resize if too large\n",
    "    width, height = image.size\n",
    "    if width > max_size or height > max_size:\n",
    "        if width > height:\n",
    "            new_width = max_size\n",
    "            new_height = int((max_size / width) * height)\n",
    "        else:\n",
    "            new_height = max_size\n",
    "            new_width = int((max_size / height) * width)\n",
    "        image = image.resize((new_width, new_height), Image.ANTIALIAS)\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]},\n",
    "        {\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": image}, {\"type\": \"text\", \"text\": \"Describe this image in detail.\"}]}\n",
    "    ]\n",
    "\n",
    "    if model is None or processor is None or tokenizer is None:\n",
    "        return \"Model not loaded in this environment. See notebook instructions.\", image\n",
    "\n",
    "    inputs = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors='pt', use_fast=True, do_sample=False)\n",
    "    inputs = {k: (v.to(model.device, dtype=torch.bfloat16) if v.is_floating_point() else v.to(model.device)) for k, v in inputs.items()}\n",
    "\n",
    "    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True)\n",
    "    generation_kwargs = dict(inputs, streamer=streamer, max_new_tokens=150)\n",
    "    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "    thread.start()\n",
    "\n",
    "    report = ''\n",
    "    for new_text in streamer:\n",
    "        if stop_flag:\n",
    "            report += '\\n ## Inference stopped.'\n",
    "            break\n",
    "        report += new_text\n",
    "        # Yield partial result to allow streaming in Gradio\n",
    "        yield report, image\n",
    "    yield report, image\n",
    "\n",
    "def stopper():\n",
    "    global stop_flag\n",
    "    stop_flag = True\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown('# Streaming Multi Model Generation')\n",
    "    gr.Markdown('Upload an image or provide an image URL for VLM analysis.')\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            image_input = gr.Image(type='numpy', label='Upload Image')\n",
    "            image_url_input = gr.Textbox(label='Image URL')\n",
    "            stop_button = gr.Button(value='Stop')\n",
    "            clear_button = gr.ClearButton([image_input, image_url_input])\n",
    "        with gr.Column():\n",
    "            output_text = gr.Markdown(label='Image Analysis Result')\n",
    "            output_image = gr.Image(label='Processed Image')\n",
    "\n",
    "    stop_button.click(fn=stopper)\n",
    "    image_input.change(fn=process_image, inputs=[image_input, image_url_input, stop_button], outputs=[output_text, output_image])\n",
    "    image_url_input.change(fn=process_image, inputs=[image_input, image_url_input, stop_button], outputs=[output_text, output_image])\n",
    "\n",
    "# Launching the demo in this notebook will block the kernel until stopped.\n",
    "# Uncomment the next line to run locally (not recommended in ephemeral cloud notebooks):\n",
    "# demo.launch(share=True, debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5739bc2",
   "metadata": {},
   "source": [
    "\n",
    "## Final notes & tips\n",
    "- Streaming is valuable for responsive UIs (chatbots, assistants).\n",
    "- For development, test with smaller models to save time and memory.\n",
    "- If you plan to run the full `google/gemma-3-4b-it`, ensure you have access and a GPU with enough memory (or use device_map/accelerate to offload).\n",
    "- Consider adding a `requirements.txt` and a short README in the repo linking back to your Medium article.\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
